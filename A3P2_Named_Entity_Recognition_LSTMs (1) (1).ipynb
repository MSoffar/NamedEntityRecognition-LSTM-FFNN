{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6czvz5VKO5M"
      },
      "source": [
        "# Notebook for Programming in Problem 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5o8HI5JqTvU5"
      },
      "source": [
        "## Learning Objectives\n",
        "In this problem, we will use [PyTorch](https://pytorch.org/) to implement long short-term memory (LSTM) for named entity recognition (NER). We will use the same dataset and boilerplate code as in Programming Problem 1 of Assignment #3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObrHyvWvTyGZ"
      },
      "source": [
        "## Writing Code\n",
        "Look for the keyword \"TODO\" and fill in your code in the empty space.\n",
        "Feel free to change function signatures, but be careful that you might need to also change how they are called in other parts of the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "r6YTnpgbFdMI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10f9d616-c932-4090-f537-2082deec9152"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Apr 30 19:10:02 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi # you may need to try reconnecting to get a T4 gpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnYMKJlKNXYe"
      },
      "source": [
        "## Installing PyTorch and Other Packages\n",
        "\n",
        "Install PyTorch using pip. See [https://pytorch.org/](https://pytorch.org/) if you want to install it on your computer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-dRVuiP_JVdT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dbae0cd-7a28-4ea2-f6e0-578c05b4f95a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.31.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.25.2)\n",
            "Requirement already satisfied: torchdata==0.7.1 in /usr/local/lib/python3.10/dist-packages (from torchtext) (0.7.1)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.7.1->torchtext) (2.0.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchtext -f https://download.pytorch.org/whl/torch_stable.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPsFH637OpLy"
      },
      "source": [
        "Test if our installation works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "c62StNb2NvKk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25e7c64c-d559-4bd2-b8fb-513850ef6e18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch successfully installed!\n",
            "Version: 2.2.1+cu121\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Multiply two matrices on GPU\n",
        "a = torch.rand(100, 200).cuda()\n",
        "b = torch.rand(200, 100).cuda()\n",
        "c = torch.matmul(a, b)\n",
        "\n",
        "print(\"PyTorch successfully installed!\")\n",
        "print(\"Version:\", torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qaC8sxcqkGX"
      },
      "source": [
        "Also install [scikit-learn](https://scikit-learn.org/stable/). We will use it for calculating evaluation metrics such as accuracy and F1 score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "i5Y2xB_uqqM9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a64ceab9-094d-47e6-81e2-85bc337e2fd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "Successfully installed scikit-learn-1.4.2\n"
          ]
        }
      ],
      "source": [
        "!pip install -U scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhV4CYivRbt4"
      },
      "source": [
        "Let's import all the packages at once:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EjRM4cCFRh-d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.vocab import Vocab, vocab\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "import re\n",
        "from collections import Counter\n",
        "from typing import List, Tuple, Dict, Optional, Any"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yn1bIPjAN-9V"
      },
      "source": [
        "## Long Short Term Memory (LSTM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJOKIneRTrTH"
      },
      "source": [
        "### Data Loading\n",
        "\n",
        "We will use the same dataset for named entity recognition in Assignment #2. First download the data and take a look at the first 50 lines:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lWqz7kDxSqeb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d99f117f-607e-4ce7-9768-ab13648b73c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EU NNP I-NP ORG\n",
            "rejects VBZ I-VP O\n",
            "German JJ I-NP MISC\n",
            "call NN I-NP O\n",
            "to TO I-VP O\n",
            "boycott VB I-VP O\n",
            "British JJ I-NP MISC\n",
            "lamb NN I-NP O\n",
            ". . O O\n",
            "\n",
            "Peter NNP I-NP PER\n",
            "Blackburn NNP I-NP PER\n",
            "\n",
            "BRUSSELS NNP I-NP LOC\n",
            "1996-08-22 CD I-NP O\n",
            "\n",
            "The DT I-NP O\n",
            "European NNP I-NP ORG\n",
            "Commission NNP I-NP ORG\n",
            "said VBD I-VP O\n",
            "on IN I-PP O\n",
            "Thursday NNP I-NP O\n",
            "it PRP B-NP O\n",
            "disagreed VBD I-VP O\n",
            "with IN I-PP O\n",
            "German JJ I-NP MISC\n",
            "advice NN I-NP O\n",
            "to TO I-PP O\n",
            "consumers NNS I-NP O\n",
            "to TO I-VP O\n",
            "shun VB I-VP O\n",
            "British JJ I-NP MISC\n",
            "lamb NN I-NP O\n",
            "until IN I-SBAR O\n",
            "scientists NNS I-NP O\n",
            "determine VBP I-VP O\n",
            "whether IN I-SBAR O\n",
            "mad JJ I-NP O\n",
            "cow NN I-NP O\n",
            "disease NN I-NP O\n",
            "can MD I-VP O\n",
            "be VB I-VP O\n",
            "transmitted VBN I-VP O\n",
            "to TO I-PP O\n",
            "sheep NN I-NP O\n",
            ". . O O\n",
            "\n",
            "Germany NNP I-NP LOC\n",
            "'s POS B-NP O\n",
            "representative NN I-NP O\n"
          ]
        }
      ],
      "source": [
        "!wget --quiet https://princeton-nlp.github.io/cos484/assignments/a2/eng.train\n",
        "!wget --quiet https://princeton-nlp.github.io/cos484/assignments/a2/eng.val\n",
        "!cat eng.train | head -n 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVt1a6nzWsiF"
      },
      "source": [
        "Each line corresponds to a word. Different sentences are separated by an additional line break. Take \"EU NNP I-NP ORG\" as an example. \"EU\" is a word. \"NNP\" and \"I-NP\" are tags for POS tagging and chunking, which we will ignore. \"ORG\" is the tag for NER, which is our prediction target. There are 5 possible values for the NER tag: ORG, PER, LOC, MISC, and O.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WnNfOBUYJvVW"
      },
      "outputs": [],
      "source": [
        "# A sentence is a list of (word, tag) tuples.\n",
        "# For example, [(\"hello\", \"O\"), (\"world\", \"O\"), (\"!\", \"O\")]\n",
        "Sentence = List[Tuple[str, str]]\n",
        "\n",
        "\n",
        "def read_data_file(\n",
        "    datapath: str,\n",
        ") -> Tuple[List[Sentence], Dict[str, int], Dict[str, int]]:\n",
        "    \"\"\"\n",
        "    Read and preprocess input data from the file `datapath`.\n",
        "    Example:\n",
        "    ```\n",
        "        sentences, word_cnt, tag_cnt = read_data_file(\"eng.train\")\n",
        "    ```\n",
        "    Return values:\n",
        "        `sentences`: a list of sentences, including words and NER tags\n",
        "        `word_cnt`: a Counter object, the number of occurrences of each word\n",
        "        `tag_cnt`: a Counter object, the number of occurences of each NER tag\n",
        "    \"\"\"\n",
        "    sentences: List[Sentence] = []\n",
        "    word_cnt: Dict[str, int] = Counter()\n",
        "    tag_cnt: Dict[str, int] = Counter()\n",
        "\n",
        "    for sentence_txt in open(datapath).read().split(\"\\n\\n\"):\n",
        "        if \"DOCSTART\" in sentence_txt:\n",
        "            # Ignore dummy sentences at the begining of each document.\n",
        "            continue\n",
        "        # Read a new sentence\n",
        "        sentences.append([])\n",
        "        for token in sentence_txt.split(\"\\n\"):\n",
        "            w, _, _, t = token.split()\n",
        "            # Replace all digits with \"0\" to reduce out-of-vocabulary words\n",
        "            w = re.sub(\"\\d\", \"0\", w)\n",
        "            word_cnt[w] += 1\n",
        "            tag_cnt[t] += 1\n",
        "            sentences[-1].append((w, t))\n",
        "\n",
        "    return sentences, word_cnt, tag_cnt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WLMGYSZ7KxzP"
      },
      "outputs": [],
      "source": [
        "# Some helper code\n",
        "def get_device() -> torch.device:\n",
        "    \"\"\"\n",
        "    Use GPU when it is available; use CPU otherwise.\n",
        "    See https://pytorch.org/docs/stable/notes/cuda.html#device-agnostic-code\n",
        "    \"\"\"\n",
        "    return torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wVHAOb7iMPwC"
      },
      "outputs": [],
      "source": [
        "def eval_metrics(ground_truth: List[int], predictions: List[int]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Calculate various evaluation metrics such as accuracy and F1 score\n",
        "    Parameters:\n",
        "        `ground_truth`: the list of ground truth NER tags\n",
        "        `predictions`: the list of predicted NER tags\n",
        "    \"\"\"\n",
        "    f1_scores = f1_score(ground_truth, predictions, average=None)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(ground_truth, predictions),\n",
        "        \"f1\": f1_scores,\n",
        "        \"average f1\": np.mean(f1_scores),\n",
        "        \"confusion matrix\": confusion_matrix(ground_truth, predictions),\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s830dhbnj1L"
      },
      "source": [
        "## Long Short-term Memory (LSTM)\n",
        "\n",
        "Now we implement an one-layer LSTM for the same task and compare it to FFNNs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "to7DnWNiY5ZS"
      },
      "source": [
        "### Data Loading **(4 points)**\n",
        "\n",
        "Like before, we first implement the data loader. But unlike before, each data example is now a variable-length sentence. How can we pack multiple sentences with different lengths into the same batch? One possible solution is to pad them to the same length using a special token. The code below illustrates the idea:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5oVgqE7JaJp"
      },
      "outputs": [],
      "source": [
        "# 3 sentences with different lengths\n",
        "sentence_1 = torch.tensor([6, 1, 2])\n",
        "sentence_2 = torch.tensor([4, 2, 7, 7, 9])\n",
        "sentence_3 = torch.tensor([3, 4])\n",
        "# Form a batch by padding 0\n",
        "sentence_batch = torch.tensor([\n",
        "    [6, 1, 2, 0, 0],\n",
        "    [4, 2, 7, 7, 9],\n",
        "    [3, 4, 0, 0, 0],\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udC0SMjkKaCN"
      },
      "source": [
        "We implement the above idea in a customized batching function `form_batch`. Optionally, see [here](https://pytorch.org/docs/stable/data.html#loading-batched-and-non-batched-data) for how batching works in PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "sACcGN4XYMgj"
      },
      "outputs": [],
      "source": [
        "class SequenceDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Each data example is a sentence, including its words and NER tags.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, datapath: str, words_vocab: Optional[Vocab] = None, tags_vocab: Optional[Vocab] = None\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize the dataset by reading from datapath.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.sentences: List[Sentence] = []\n",
        "        UNKNOWN = \"<UNKNOWN>\"\n",
        "        PAD = \"<PAD>\"  # Special token used for padding\n",
        "\n",
        "        print(\"Loading data from %s\" % datapath)\n",
        "        self.sentences, word_cnt, tag_cnt = read_data_file(datapath)\n",
        "        print(\"%d sentences loaded.\" % len(self.sentences))\n",
        "\n",
        "        if words_vocab is None:\n",
        "            words_vocab = vocab(word_cnt, specials=[PAD, UNKNOWN])\n",
        "            words_vocab.set_default_index(words_vocab[UNKNOWN])\n",
        "\n",
        "        self.words_vocab = words_vocab\n",
        "\n",
        "        self.unknown_idx = self.words_vocab[UNKNOWN]\n",
        "        self.pad_idx = self.words_vocab[PAD]\n",
        "\n",
        "        if tags_vocab is None:\n",
        "            tags_vocab = vocab(tag_cnt, specials=[])\n",
        "        self.tags_vocab = tags_vocab\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Sentence:\n",
        "        \"\"\"\n",
        "        Get the idx'th sentence in the dataset.\n",
        "        \"\"\"\n",
        "        return self.sentences[idx]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"\n",
        "        Return the number of sentences in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def form_batch(self, sentences: List[Sentence]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        A customized function for batching a number of sentences together.\n",
        "        Different sentences have different lengths. Let max_len be the longest length.\n",
        "        When packing them into one tensor, we need to pad all sentences to max_len.\n",
        "        Return values:\n",
        "            `words`: a list in which each element itself is a list of words in a sentence\n",
        "            `word_idxs`: a batch_size x max_len tensor.\n",
        "                       word_idxs[i][j] is the index of the j'th word in the i'th sentence .\n",
        "            `tags`: a list in which each element itself is a list of tags in a sentence\n",
        "            `tag_idxs`: a batch_size x max_len tensor\n",
        "                      tag_idxs[i][j] is the index of the j'th tag in the i'th sentence.\n",
        "            `valid_mask`: a batch_size x max_len tensor\n",
        "                        valid_mask[i][j] is True if the i'th sentence has the j'th word.\n",
        "                        Otherwise, valid[i][j] is False.\n",
        "        \"\"\"\n",
        "        words: List[List[str]] = []\n",
        "        tags: List[List[str]] = []\n",
        "        max_len = -1  # length of the longest sentence\n",
        "        for sent in sentences:\n",
        "            words.append([])\n",
        "            tags.append([])\n",
        "            for w, t in sent:\n",
        "                words[-1].append(w)\n",
        "                tags[-1].append(t)\n",
        "            max_len = max(max_len, len(words[-1]))\n",
        "\n",
        "        batch_size = len(sentences)\n",
        "        word_idxs = torch.full(\n",
        "            (batch_size, max_len), fill_value=self.pad_idx, dtype=torch.int64\n",
        "        )\n",
        "        tag_idxs = torch.full_like(word_idxs, fill_value=self.tags_vocab[\"O\"])\n",
        "        valid_mask = torch.zeros_like(word_idxs, dtype=torch.bool)\n",
        "\n",
        "        ## Fill in the values in word_idxs, tag_idxs, and valid_mask\n",
        "        ## Caveat: There may be out-of-vocabulary words in validation data\n",
        "        for i, (w_seq, t_seq) in enumerate(zip(words, tags)):\n",
        "            for j, (w, t) in enumerate(zip(w_seq, t_seq)):\n",
        "                word_idxs[i][j] = self.words_vocab[w] if w in self.words_vocab else self.unknown_idx\n",
        "                tag_idxs[i][j] = self.tags_vocab[t]\n",
        "                valid_mask[i][j] = True\n",
        "\n",
        "        return {\n",
        "            \"words\": words,\n",
        "            \"word_idxs\": word_idxs,\n",
        "            \"tags\": tags,\n",
        "            \"tag_idxs\": tag_idxs,\n",
        "            \"valid_mask\": valid_mask,\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "def create_sequence_dataloaders(\n",
        "    batch_size: int, shuffle: bool = True\n",
        ") -> Tuple[DataLoader, DataLoader, Vocab]:\n",
        "    \"\"\"\n",
        "    Create the dataloaders for training and validaiton.\n",
        "    \"\"\"\n",
        "    ds_train = SequenceDataset(\"eng.train\")\n",
        "    ds_val = SequenceDataset(\"eng.val\", words_vocab=ds_train.words_vocab, tags_vocab=ds_train.tags_vocab)\n",
        "    loader_train = DataLoader(\n",
        "        ds_train,\n",
        "        batch_size,\n",
        "        shuffle,\n",
        "        collate_fn=ds_train.form_batch,  # customized function for batching\n",
        "        drop_last=True,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "    loader_val = DataLoader(\n",
        "        ds_val, batch_size, collate_fn=ds_val.form_batch, pin_memory=True\n",
        "    )\n",
        "    return loader_train, loader_val, ds_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2EcVxYuYvGv"
      },
      "source": [
        "Here is a simple sanity-check. Try to understand its output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "TazmodGWYx2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e3a791c-993c-4d27-f198-dc6f41e3f954"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from eng.train\n",
            "14041 sentences loaded.\n",
            "Loading data from eng.val\n",
            "3490 sentences loaded.\n",
            "Iterating on the training data..\n",
            "{'words': [['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], ['Peter', 'Blackburn'], ['BRUSSELS', '0000-00-00']], 'word_idxs': tensor([[ 2,  3,  4,  5,  6,  7,  8,  9, 10],\n",
            "        [11, 12,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [13, 14,  0,  0,  0,  0,  0,  0,  0]]), 'tags': [['ORG', 'O', 'MISC', 'O', 'O', 'O', 'MISC', 'O', 'O'], ['PER', 'PER'], ['LOC', 'O']], 'tag_idxs': tensor([[0, 1, 2, 1, 1, 1, 2, 1, 1],\n",
            "        [3, 3, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [4, 1, 1, 1, 1, 1, 1, 1, 1]]), 'valid_mask': tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
            "        [ True,  True, False, False, False, False, False, False, False],\n",
            "        [ True,  True, False, False, False, False, False, False, False]])}\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "def check_sequence_dataloader() -> None:\n",
        "    loader_train, _, _ = create_sequence_dataloaders(batch_size=3, shuffle=False)\n",
        "    print(\"Iterating on the training data..\")\n",
        "    for i, data_batch in enumerate(loader_train):\n",
        "        if i == 0:\n",
        "            print(data_batch)\n",
        "    print(\"Done!\")\n",
        "\n",
        "\n",
        "check_sequence_dataloader()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifk3i-obY8YB"
      },
      "source": [
        "### Implement the Model **(8 points)**\n",
        "\n",
        "Next, implement LSTM for predicting NER tags from input words. [nn.LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM) is definitely useful. Further, it is tricky to handle sentences in the same batch with different lengths. Please read the PyTorch documentation in detail!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "3V0NvQynZF8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "364d7159-9e00-44f7-d703-91360f356912"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from eng.train\n",
            "14041 sentences loaded.\n",
            "Loading data from eng.val\n",
            "3490 sentences loaded.\n"
          ]
        }
      ],
      "source": [
        "class SequenceDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Each data example is a sentence, including its words and NER tags.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, datapath: str, words_vocab: Optional[Vocab] = None, tags_vocab: Optional[Vocab] = None\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize the dataset by reading from datapath.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.sentences: List[Sentence] = []\n",
        "        UNKNOWN = \"<UNKNOWN>\"\n",
        "        PAD = \"<PAD>\"  # Special token used for padding\n",
        "\n",
        "        print(\"Loading data from %s\" % datapath)\n",
        "        self.sentences, word_cnt, tag_cnt = read_data_file(datapath)\n",
        "        print(\"%d sentences loaded.\" % len(self.sentences))\n",
        "\n",
        "        if words_vocab is None:\n",
        "            words_vocab = vocab(word_cnt, specials=[PAD, UNKNOWN])\n",
        "            words_vocab.set_default_index(words_vocab[UNKNOWN])\n",
        "\n",
        "        self.words_vocab = words_vocab\n",
        "\n",
        "        self.unknown_idx = self.words_vocab[UNKNOWN]\n",
        "        self.pad_idx = self.words_vocab[PAD]\n",
        "\n",
        "        if tags_vocab is None:\n",
        "            tags_vocab = vocab(tag_cnt, specials=[])\n",
        "        self.tags_vocab = tags_vocab\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Sentence:\n",
        "        \"\"\"\n",
        "        Get the idx'th sentence in the dataset.\n",
        "        \"\"\"\n",
        "        return self.sentences[idx]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"\n",
        "        Return the number of sentences in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def form_batch(self, sentences: List[Sentence]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        A customized function for batching a number of sentences together.\n",
        "        Different sentences have different lengths. Let max_len be the longest length.\n",
        "        When packing them into one tensor, we need to pad all sentences to max_len.\n",
        "        Return values:\n",
        "            `words`: a list in which each element itself is a list of words in a sentence\n",
        "            `word_idxs`: a batch_size x max_len tensor.\n",
        "                       word_idxs[i][j] is the index of the j'th word in the i'th sentence .\n",
        "            `tags`: a list in which each element itself is a list of tags in a sentence\n",
        "            `tag_idxs`: a batch_size x max_len tensor\n",
        "                      tag_idxs[i][j] is the index of the j'th tag in the i'th sentence.\n",
        "            `valid_mask`: a batch_size x max_len tensor\n",
        "                        valid_mask[i][j] is True if the i'th sentence has the j'th word.\n",
        "                        Otherwise, valid[i][j] is False.\n",
        "        \"\"\"\n",
        "        words: List[List[str]] = []\n",
        "        tags: List[List[str]] = []\n",
        "        max_len = -1  # length of the longest sentence\n",
        "        for sent in sentences:\n",
        "            words.append([])\n",
        "            tags.append([])\n",
        "            for w, t in sent:\n",
        "                words[-1].append(w)\n",
        "                tags[-1].append(t)\n",
        "            max_len = max(max_len, len(words[-1]))\n",
        "\n",
        "        batch_size = len(sentences)\n",
        "        word_idxs = torch.full(\n",
        "            (batch_size, max_len), fill_value=self.pad_idx, dtype=torch.int64\n",
        "        )\n",
        "        tag_idxs = torch.full_like(word_idxs, fill_value=self.tags_vocab[\"O\"])\n",
        "        valid_mask = torch.zeros_like(word_idxs, dtype=torch.bool)\n",
        "\n",
        "        ## Fill in the values in word_idxs, tag_idxs, and valid_mask\n",
        "        ## Caveat: There may be out-of-vocabulary words in validation data\n",
        "        for i, (w_seq, t_seq) in enumerate(zip(words, tags)):\n",
        "            for j, (w, t) in enumerate(zip(w_seq, t_seq)):\n",
        "                word_idxs[i][j] = self.words_vocab[w] if w in self.words_vocab else self.unknown_idx\n",
        "                tag_idxs[i][j] = self.tags_vocab[t]\n",
        "                valid_mask[i][j] = True\n",
        "\n",
        "        return {\n",
        "            \"words\": words,\n",
        "            \"word_idxs\": word_idxs,\n",
        "            \"tags\": tags,\n",
        "            \"tag_idxs\": tag_idxs,\n",
        "            \"valid_mask\": valid_mask,\n",
        "        }\n",
        "\n",
        "\n",
        "def create_sequence_dataloaders(\n",
        "    batch_size: int, shuffle: bool = True\n",
        ") -> Tuple[DataLoader, DataLoader, Vocab]:\n",
        "    \"\"\"\n",
        "    Create the dataloaders for training and validation.\n",
        "    \"\"\"\n",
        "    ds_train = SequenceDataset(\"eng.train\")\n",
        "    ds_val = SequenceDataset(\"eng.val\", words_vocab=ds_train.words_vocab, tags_vocab=ds_train.tags_vocab)\n",
        "    loader_train = DataLoader(\n",
        "        ds_train,\n",
        "        batch_size,\n",
        "        shuffle,\n",
        "        collate_fn=ds_train.form_batch,  # customized function for batching\n",
        "        drop_last=True,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "    loader_val = DataLoader(\n",
        "        ds_val, batch_size, collate_fn=ds_val.form_batch, pin_memory=True\n",
        "    )\n",
        "    return loader_train, loader_val, ds_train\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    Long short-term memory for NER\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, words_vocab: Vocab, tags_vocab:Vocab, d_emb: int, d_hidden: int, bidirectional: bool) -> None:\n",
        "        \"\"\"\n",
        "        Initialize an LSTM\n",
        "        Parameters:\n",
        "            `words_vocab`: vocabulary of words\n",
        "            `tags_vocab`: vocabulary of tags\n",
        "            `d_emb`: dimension of word embeddings (D)\n",
        "            `d_hidden`: dimension of the hidden layer (H)\n",
        "            `bidirectional`: true if LSTM should be bidirectional\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.words_vocab = words_vocab\n",
        "        self.tags_vocab = tags_vocab\n",
        "        self.d_emb = d_emb\n",
        "        self.d_hidden = d_hidden\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        # Create word embeddings\n",
        "        self.embedding = nn.Embedding(len(words_vocab), d_emb)\n",
        "\n",
        "        # Create LSTM layer\n",
        "        self.lstm = nn.LSTM(\n",
        "            d_emb,\n",
        "            d_hidden,\n",
        "            bidirectional=bidirectional,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Create output layer\n",
        "        self.output_layer = nn.Linear(\n",
        "            d_hidden * 2 if bidirectional else d_hidden,\n",
        "            len(tags_vocab)\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self, word_idxs: torch.Tensor, valid_mask: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Given words in sentences, predict the logits of the NER tag.\n",
        "        Parameters:\n",
        "            `word_idxs`: a batch_size x max_len tensor\n",
        "            `valid_mask`: a batch_size x max_len tensor\n",
        "        Return values:\n",
        "            `logits`: a batch_size x max_len x 5 tensor\n",
        "        \"\"\"\n",
        "        # Get word embeddings\n",
        "        word_embeddings = self.embedding(word_idxs)\n",
        "\n",
        "        # Pack padded sequence\n",
        "        packed_word_embeddings = nn.utils.rnn.pack_padded_sequence(\n",
        "            word_embeddings, valid_mask.sum(dim=1).cpu(), batch_first=True, enforce_sorted=False\n",
        "        )\n",
        "\n",
        "        # Apply LSTM\n",
        "        packed_output, _ = self.lstm(packed_word_embeddings)\n",
        "\n",
        "        # Unpack padded sequence\n",
        "        lstm_output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
        "\n",
        "        # Apply output layer\n",
        "        logits = self.output_layer(lstm_output)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "loader_train, loader_val, ds_train = create_sequence_dataloaders(batch_size=32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BFTKaB4Zydx"
      },
      "source": [
        "We do a sanity-check by loading a batch of data examples and pass it through the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "PKg1ni4QZ6D1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0095823-fa91-4c46-9bfa-2c4bbff2cd91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from eng.train\n",
            "14041 sentences loaded.\n",
            "Loading data from eng.val\n",
            "3490 sentences loaded.\n",
            "LSTM(\n",
            "  (words_vocab): Vocab()\n",
            "  (tags_vocab): Vocab()\n",
            "  (embedding): Embedding(20102, 64)\n",
            "  (lstm): LSTM(64, 128, batch_first=True, bidirectional=True)\n",
            "  (output_layer): Linear(in_features=256, out_features=5, bias=True)\n",
            ")\n",
            "Input word_idxs shape: torch.Size([4, 15])\n",
            "Input valid_mask shape: torch.Size([4, 15])\n",
            "Output logits shape: torch.Size([4, 15, 5])\n"
          ]
        }
      ],
      "source": [
        "def check_lstm() -> None:\n",
        "    # Hyperparameters\n",
        "    batch_size = 4\n",
        "    d_emb = 64\n",
        "    d_hidden = 128\n",
        "    bidirectional = True\n",
        "    # Create the dataloaders and the model\n",
        "    loader_train, _, ds_train = create_sequence_dataloaders(batch_size)\n",
        "    model = LSTM(ds_train.words_vocab, ds_train.tags_vocab, d_emb, d_hidden, bidirectional)\n",
        "    device = get_device()\n",
        "    model.to(device)\n",
        "    print(model)\n",
        "    # Get the first batch\n",
        "    data_batch = next(iter(loader_train))\n",
        "    # Move data to GPU\n",
        "    word_idxs = data_batch[\"word_idxs\"].to(device, non_blocking=True)\n",
        "    tag_idxs = data_batch[\"tag_idxs\"].to(device, non_blocking=True)\n",
        "    valid_mask = data_batch[\"valid_mask\"].to(device, non_blocking=True)\n",
        "    # Calculate the model\n",
        "    print(\"Input word_idxs shape:\", word_idxs.size())\n",
        "    print(\"Input valid_mask shape:\", valid_mask.size())\n",
        "    logits = model(word_idxs, valid_mask)\n",
        "    print(\"Output logits shape:\", logits.size())\n",
        "\n",
        "\n",
        "check_lstm()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jddDYUiLY-hc"
      },
      "source": [
        "### Training and Validation **(6 points)**\n",
        "\n",
        "Complete the functions for training and validating the LSTM model. When calculating the loss function, you only want to include values from valid positions (where `valid_mask` is `True`). The `reduction` parameter in [F.cross_entropy](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.cross_entropy) may be useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "hv_15mnXZ_dy"
      },
      "outputs": [],
      "source": [
        "def train_lstm(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    optimizer: optim.Optimizer,\n",
        "    device: torch.device,\n",
        "    silent: bool = False,  # whether to print the training loss\n",
        ") -> Tuple[float, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Train the LSTM model.\n",
        "    Return values:\n",
        "        1. the average training loss\n",
        "        2. training metrics such as accuracy and F1 score\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    ground_truth = []\n",
        "    predictions = []\n",
        "    losses = []\n",
        "    report_interval = 100\n",
        "\n",
        "    for i, data_batch in enumerate(loader):\n",
        "        word_idxs = data_batch[\"word_idxs\"].to(device, non_blocking=True)\n",
        "        tag_idxs = data_batch[\"tag_idxs\"].to(device, non_blocking=True)\n",
        "        valid_mask = data_batch[\"valid_mask\"].to(device, non_blocking=True)\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(word_idxs, valid_mask)\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), tag_idxs.view(-1), reduction='none')\n",
        "        loss = (loss * valid_mask.view(-1)).sum() / valid_mask.sum()  # only include valid positions\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # we get (unmasked) predictions by getting argmax of logits along last dimension\n",
        "        net_predictions = torch.argmax(logits, -1)\n",
        "\n",
        "        # Flatten tensors to make it easier to extract ground truths and predictions\n",
        "        tag_idxs_flat = tag_idxs.flatten()\n",
        "        valid_mask_flat = valid_mask.flatten()\n",
        "        net_predictions_flat = net_predictions.flatten()\n",
        "\n",
        "        ground_truth.extend(tag_idxs_flat[valid_mask_flat].tolist())\n",
        "        predictions.extend(net_predictions_flat[valid_mask_flat].tolist())\n",
        "\n",
        "        if not silent and i > 0 and i % report_interval == 0:\n",
        "            print(\n",
        "                \"\\t[%06d/%06d] Loss: %f\"\n",
        "                % (i, len(loader), np.mean(losses[-report_interval:]))\n",
        "            )\n",
        "\n",
        "    return np.mean(losses), eval_metrics(ground_truth, predictions)\n",
        "\n",
        "\n",
        "def validate_lstm(\n",
        "    model: nn.Module, loader: DataLoader, device: torch.device\n",
        ") -> Tuple[float, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Validate the model.\n",
        "    Return the validation loss and metrics.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    ground_truth = []\n",
        "    predictions = []\n",
        "    losses = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for data_batch in loader:\n",
        "            word_idxs = data_batch[\"word_idxs\"].to(device, non_blocking=True)\n",
        "            tag_idxs = data_batch[\"tag_idxs\"].to(device, non_blocking=True)\n",
        "            valid_mask = data_batch[\"valid_mask\"].to(device, non_blocking=True)\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model(word_idxs, valid_mask)\n",
        "\n",
        "            # Calculate the loss\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), tag_idxs.view(-1), reduction='none')\n",
        "            loss = (loss * valid_mask.view(-1)).sum() / valid_mask.sum()  # only include valid positions\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            # we get (unmasked) predictions by getting argmax of logits\n",
        "            net_predictions = torch.argmax(logits, -1)\n",
        "\n",
        "            # Flatten tensors to make it easier to extract ground truths and predictions\n",
        "            tag_idxs_flat = tag_idxs.flatten()\n",
        "            valid_mask_flat = valid_mask.flatten()\n",
        "            net_predictions_flat = net_predictions.flatten()\n",
        "\n",
        "            ground_truth.extend(tag_idxs_flat[valid_mask_flat].tolist())\n",
        "            predictions.extend(net_predictions_flat[valid_mask_flat].tolist())\n",
        "\n",
        "    return np.mean(losses), eval_metrics(ground_truth, predictions)\n",
        "\n",
        "\n",
        "def train_val_loop_lstm(hyperparams: Dict[str, Any]) -> None:\n",
        "    \"\"\"\n",
        "    Train and validate the LSTM model for a number of epochs.\n",
        "    \"\"\"\n",
        "    print(\"Hyperparameters:\", hyperparams)\n",
        "    # Create the dataloaders\n",
        "    loader_train, loader_val, ds_train = create_sequence_dataloaders(\n",
        "        hyperparams[\"batch_size\"]\n",
        "    )\n",
        "    # Create the model\n",
        "    model = LSTM(\n",
        "        ds_train.words_vocab,\n",
        "        ds_train.tags_vocab,\n",
        "        hyperparams[\"d_emb\"],\n",
        "        hyperparams[\"d_hidden\"],\n",
        "        hyperparams[\"bidirectional\"],\n",
        "    )\n",
        "    device = get_device()\n",
        "    model.to(device)\n",
        "    print(model)\n",
        "    # Create the optimizer\n",
        "    optimizer = optim.RMSprop(\n",
        "        model.parameters(), hyperparams[\"learning_rate\"], weight_decay=hyperparams[\"l2\"]\n",
        "    )\n",
        "\n",
        "    # Train and validate\n",
        "    for i in range(hyperparams[\"num_epochs\"]):\n",
        "        print(\"Epoch #%d\" % i)\n",
        "\n",
        "        print(\"Training..\")\n",
        "        loss_train, metrics_train = train_lstm(model, loader_train, optimizer, device)\n",
        "        print(\"Training loss: \", loss_train)\n",
        "        print(\"Training metrics:\")\n",
        "        for k, v in metrics_train.items():\n",
        "            print(\"\\t\", k, \": \", v)\n",
        "\n",
        "        print(\"Validating..\")\n",
        "        loss_val, metrics_val = validate_lstm(model, loader_val, device)\n",
        "        print(\"Validation loss: \", loss_val)\n",
        "        print(\"Validation metrics:\")\n",
        "        for k, v in metrics_val.items():\n",
        "            print(\"\\t\", k, \": \", v)\n",
        "\n",
        "    print(\"Done!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU9Nef7yal_M"
      },
      "source": [
        "Run the experiment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "pFxQxlokai6Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b7531ab-cc22-479c-fd92-6f5729f20865"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameters: {'bidirectional': True, 'batch_size': 512, 'd_emb': 64, 'd_hidden': 128, 'num_epochs': 15, 'learning_rate': 0.005, 'l2': 1e-06}\n",
            "Loading data from eng.train\n",
            "14041 sentences loaded.\n",
            "Loading data from eng.val\n",
            "3490 sentences loaded.\n",
            "LSTM(\n",
            "  (words_vocab): Vocab()\n",
            "  (tags_vocab): Vocab()\n",
            "  (embedding): Embedding(20102, 64)\n",
            "  (lstm): LSTM(64, 128, batch_first=True, bidirectional=True)\n",
            "  (output_layer): Linear(in_features=256, out_features=5, bias=True)\n",
            ")\n",
            "Epoch #0\n",
            "Training..\n",
            "Training loss:  0.722828416912644\n",
            "Training metrics:\n",
            "\t accuracy :  0.8019797532047442\n",
            "\t f1 :  [0.19807956 0.89360102 0.06097323 0.27550492 0.25591413]\n",
            "\t average f1 :  0.3368145720528884\n",
            "\t confusion matrix :  [[  1805   7190     37    477    399]\n",
            " [  5529 154492    355   3194   3254]\n",
            " [   209   3708    156    123    329]\n",
            " [   466   7824     20   2394    223]\n",
            " [   308   5736     24    264   1812]]\n",
            "Validating..\n",
            "Validation loss:  0.31142561776297434\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9066128835105732\n",
            "\t f1 :  [0.52089512 0.95794112 0.35798817 0.64558296 0.61411549]\n",
            "\t average f1 :  0.6193045730057336\n",
            "\t confusion matrix :  [[  966   953    29   169   133]\n",
            " [  154 40758    26   190    36]\n",
            " [   72   559   242    48    86]\n",
            " [  143   972     6  1531    38]\n",
            " [  124   689    42   115  1005]]\n",
            "Epoch #1\n",
            "Training..\n",
            "Training loss:  0.2230291454880326\n",
            "Training metrics:\n",
            "\t accuracy :  0.9326746013923198\n",
            "\t f1 :  [0.66414445 0.97312524 0.58691486 0.74804992 0.75361755]\n",
            "\t average f1 :  0.7451704043253125\n",
            "\t confusion matrix :  [[  5793   2499    236    770    571]\n",
            " [   407 165478    137    603    238]\n",
            " [   444   1260   2144    213    448]\n",
            " [   516   2553     56   7672    161]\n",
            " [   416   1443    224    296   5807]]\n",
            "Validating..\n",
            "Validation loss:  0.1968301683664322\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9389438943894389\n",
            "\t f1 :  [0.6624377  0.97627086 0.62458101 0.79011873 0.7984252 ]\n",
            "\t average f1 :  0.7703666977168613\n",
            "\t confusion matrix :  [[ 1462   444    71   122   151]\n",
            " [  319 40484    67   234    60]\n",
            " [   79   257   559    51    61]\n",
            " [  182   384    19  2063    42]\n",
            " [  122   203    67    62  1521]]\n",
            "Epoch #2\n",
            "Training..\n",
            "Training loss:  0.11419648263189527\n",
            "Training metrics:\n",
            "\t accuracy :  0.9665393735180332\n",
            "\t f1 :  [0.81572869 0.98941077 0.77123092 0.88967212 0.87243466]\n",
            "\t average f1 :  0.86769543040926\n",
            "\t confusion matrix :  [[  7769    988    259    459    380]\n",
            " [   301 166035    105    278    130]\n",
            " [   366    542   3233    135    261]\n",
            " [   383    781     61   9592    104]\n",
            " [   374    429    189    178   6993]]\n",
            "Validating..\n",
            "Validation loss:  0.1884162234408515\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9387605427209388\n",
            "\t f1 :  [0.66484815 0.97709792 0.69728832 0.80398447 0.83713355]\n",
            "\t average f1 :  0.7960704833355623\n",
            "\t confusion matrix :  [[ 1828   160    38   153    71]\n",
            " [  805 39699    78   529    53]\n",
            " [  176    95   630    78    28]\n",
            " [  187    92    15  2381    15]\n",
            " [  253    49    39    92  1542]]\n",
            "Epoch #3\n",
            "Training..\n",
            "Training loss:  0.06601038492388195\n",
            "Training metrics:\n",
            "\t accuracy :  0.9815193844774781\n",
            "\t f1 :  [0.89128759 0.99482888 0.85674548 0.95023243 0.92343644]\n",
            "\t average f1 :  0.9233061644932876\n",
            "\t confusion matrix :  [[  8670    489    180    254    265]\n",
            " [   204 166795     76    117     67]\n",
            " [   254    302   3696     80    186]\n",
            " [   192    285     33  10425     54]\n",
            " [   277    194    125     77   7508]]\n",
            "Validating..\n",
            "Validation loss:  0.17295385152101517\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9453204579717231\n",
            "\t f1 :  [0.6935123  0.97926106 0.724909   0.82887886 0.85877241]\n",
            "\t average f1 :  0.8170667257143469\n",
            "\t confusion matrix :  [[ 1860   159    45   125    61]\n",
            " [  721 39876   112   420    35]\n",
            " [  136    93   697    64    17]\n",
            " [  180    95    14  2388    13]\n",
            " [  217    54    48    75  1581]]\n",
            "Epoch #4\n",
            "Training..\n",
            "Training loss:  0.03960264046435003\n",
            "Training metrics:\n",
            "\t accuracy :  0.9893866395347677\n",
            "\t f1 :  [0.93805219 0.99709042 0.9097301  0.97839759 0.95076923]\n",
            "\t average f1 :  0.9548079079044284\n",
            "\t confusion matrix :  [[  9184    276    118     98    150]\n",
            " [   138 166720     44     48     42]\n",
            " [   153    201   4011     40    124]\n",
            " [    80    111     21  10734     35]\n",
            " [   200    113     95     41   7725]]\n",
            "Validating..\n",
            "Validation loss:  0.16742198914289474\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9494764291243939\n",
            "\t f1 :  [0.70431018 0.98059727 0.73587951 0.85868755 0.86093047]\n",
            "\t average f1 :  0.8280809940799421\n",
            "\t confusion matrix :  [[ 1871   168    34    68   109]\n",
            " [  704 40027   102   231   100]\n",
            " [  133   103   684    31    56]\n",
            " [  221   118    11  2303    37]\n",
            " [  134    58    21    41  1721]]\n",
            "Epoch #5\n",
            "Training..\n",
            "Training loss:  0.02479351374010245\n",
            "Training metrics:\n",
            "\t accuracy :  0.9938321773859801\n",
            "\t f1 :  [0.96520945 0.99823189 0.94646874 0.98778598 0.97222052]\n",
            "\t average f1 :  0.9739833156160111\n",
            "\t confusion matrix :  [[  9516    174     58     57     86]\n",
            " [    89 166832     27     25     27]\n",
            " [    82    131   4208     26     78]\n",
            " [    56     47     12  10837     19]\n",
            " [    84     71     62     26   7927]]\n",
            "Validating..\n",
            "Validation loss:  0.18459363281726837\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9471336022491138\n",
            "\t f1 :  [0.69593263 0.97979351 0.75131165 0.84376117 0.86571965]\n",
            "\t average f1 :  0.8273037210414632\n",
            "\t confusion matrix :  [[ 1942   129    37    87    55]\n",
            " [  803 39858   102   351    50]\n",
            " [  148    76   716    47    20]\n",
            " [  220    84    10  2360    16]\n",
            " [  218    49    34    59  1615]]\n",
            "Epoch #6\n",
            "Training..\n",
            "Training loss:  0.016505111491790524\n",
            "Training metrics:\n",
            "\t accuracy :  0.9960445524059397\n",
            "\t f1 :  [0.97526789 0.99884489 0.96731587 0.99344668 0.9830011 ]\n",
            "\t average f1 :  0.9835752853522399\n",
            "\t confusion matrix :  [[  9602    122     42     44     68]\n",
            " [    63 166890     27      9     15]\n",
            " [    56     86   4321      8     38]\n",
            " [    28     24      6  10839     11]\n",
            " [    64     40     29     13   8038]]\n",
            "Validating..\n",
            "Validation loss:  0.19692547832216536\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9437925274008883\n",
            "\t f1 :  [0.66433217 0.97858295 0.74905559 0.85359167 0.86316366]\n",
            "\t average f1 :  0.8217452051205474\n",
            "\t confusion matrix :  [[ 1992   120    28    67    43]\n",
            " [ 1001 39729    98   303    33]\n",
            " [  195    71   694    34    13]\n",
            " [  264    73     5  2335    13]\n",
            " [  295    40    21    42  1577]]\n",
            "Epoch #7\n",
            "Training..\n",
            "Training loss:  0.010087771116997357\n",
            "Training metrics:\n",
            "\t accuracy :  0.9979616551957857\n",
            "\t f1 :  [0.98871286 0.99931196 0.98178587 0.9974903  0.99106924]\n",
            "\t average f1 :  0.9916740479532237\n",
            "\t confusion matrix :  [[  9767     73     16     11     20]\n",
            " [    28 167026     19      3     16]\n",
            " [    33     51   4420      4     24]\n",
            " [    12     11      2  10930      6]\n",
            " [    30     29     15      6   8101]]\n",
            "Validating..\n",
            "Validation loss:  0.17006794789007731\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9554659169620666\n",
            "\t f1 :  [0.75570962 0.98237385 0.75560188 0.86009424 0.87638749]\n",
            "\t average f1 :  0.8460334150942762\n",
            "\t confusion matrix :  [[ 1853   185    38    83    91]\n",
            " [  451 40212   118   289    94]\n",
            " [   91   116   725    38    37]\n",
            " [  150   123    14  2373    30]\n",
            " [  109    67    17    45  1737]]\n",
            "Epoch #8\n",
            "Training..\n",
            "Training loss:  0.006414532920138704\n",
            "Training metrics:\n",
            "\t accuracy :  0.9988587148830557\n",
            "\t f1 :  [0.99362929 0.99958108 0.99091916 0.99872507 0.99498225]\n",
            "\t average f1 :  0.9955673689897544\n",
            "\t confusion matrix :  [[  9826     44      8     10      9]\n",
            " [    19 167025      8      1      8]\n",
            " [    13     34   4474      0     12]\n",
            " [     5      5      0  10967      5]\n",
            " [    18     21      7      2   8130]]\n",
            "Validating..\n",
            "Validation loss:  0.17244577407836914\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9571771992014017\n",
            "\t f1 :  [0.78890456 0.98286215 0.70662461 0.86879304 0.87163155]\n",
            "\t average f1 :  0.8437631828874954\n",
            "\t confusion matrix :  [[ 1678   233   107    92   140]\n",
            " [  177 40346   255   268   118]\n",
            " [   35   125   784    35    28]\n",
            " [   62   156    33  2397    42]\n",
            " [   52    75    33    36  1779]]\n",
            "Epoch #9\n",
            "Training..\n",
            "Training loss:  0.0046715095328787965\n",
            "Training metrics:\n",
            "\t accuracy :  0.9992415850871679\n",
            "\t f1 :  [0.99579938 0.99971835 0.99457064 0.99917936 0.99633476]\n",
            "\t average f1 :  0.9971204970013229\n",
            "\t confusion matrix :  [[  9838     28      5      3     13]\n",
            " [    13 166827      6      2      7]\n",
            " [     7     20   4488      2      5]\n",
            " [     5      1      0  10958      4]\n",
            " [     9     17      4      1   8155]]\n",
            "Validating..\n",
            "Validation loss:  0.19321872081075395\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9505357943201728\n",
            "\t f1 :  [0.73300038 0.98060038 0.72917658 0.85116279 0.87040515]\n",
            "\t average f1 :  0.8328690583285437\n",
            "\t confusion matrix :  [[ 1908   149    56    79    58]\n",
            " [  568 39983   207   358    48]\n",
            " [  102    95   766    33    11]\n",
            " [  182    97    19  2379    13]\n",
            " [  196    60    46    51  1622]]\n",
            "Epoch #10\n",
            "Training..\n",
            "Training loss:  0.035303838499304324\n",
            "Training metrics:\n",
            "\t accuracy :  0.9904380043141328\n",
            "\t f1 :  [0.96968468 0.99533868 0.95727253 0.96880717 0.96311525]\n",
            "\t average f1 :  0.9708436616034568\n",
            "\t confusion matrix :  [[  9564    174     45     27     28]\n",
            " [   234 165914    100    203    334]\n",
            " [    37     97   4324     20     24]\n",
            " [    26    301     41  10591     26]\n",
            " [    27    111     22     38   7964]]\n",
            "Validating..\n",
            "Validation loss:  0.1528307454926627\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9593977916310149\n",
            "\t f1 :  [0.78092399 0.98374429 0.75458602 0.87051939 0.8915912 ]\n",
            "\t average f1 :  0.8562729787473133\n",
            "\t confusion matrix :  [[ 1834   204    58    65    89]\n",
            " [  322 40395   146   246    55]\n",
            " [   63   124   761    46    13]\n",
            " [  132   151    17  2380    10]\n",
            " [   96    87    28    41  1723]]\n",
            "Epoch #11\n",
            "Training..\n",
            "Training loss:  0.005469573092543417\n",
            "Training metrics:\n",
            "\t accuracy :  0.9989020422015052\n",
            "\t f1 :  [0.99446279 0.99949971 0.99257782 0.99831243 0.99631947]\n",
            "\t average f1 :  0.9962344435332341\n",
            "\t confusion matrix :  [[  9788     43     12      5      4]\n",
            " [    29 166819     10      7     14]\n",
            " [     6     29   4480      2      3]\n",
            " [     5     10      2  10944      4]\n",
            " [     5     25      3      2   8121]]\n",
            "Validating..\n",
            "Validation loss:  0.15489468829972403\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9613942875769058\n",
            "\t f1 :  [0.79567042 0.98398225 0.76774848 0.87648368 0.88976378]\n",
            "\t average f1 :  0.8627297215946077\n",
            "\t confusion matrix :  [[ 1801   258    51    66    74]\n",
            " [  228 40575   118   197    46]\n",
            " [   50   150   757    38    12]\n",
            " [   96   210    13  2363     8]\n",
            " [  102   114    26    38  1695]]\n",
            "Epoch #12\n",
            "Training..\n",
            "Training loss:  0.0030369738642885176\n",
            "Training metrics:\n",
            "\t accuracy :  0.9996204990437575\n",
            "\t f1 :  [0.9984252  0.9998141  0.99712389 0.9994997  0.99865031]\n",
            "\t average f1 :  0.9987026405064647\n",
            "\t confusion matrix :  [[  9827     14      1      2      1]\n",
            " [     7 166726      3      2      5]\n",
            " [     1     19   4507      0      1]\n",
            " [     2      3      0  10988      2]\n",
            " [     3      9      1      0   8139]]\n",
            "Validating..\n",
            "Validation loss:  0.16075434535741806\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9622703011041845\n",
            "\t f1 :  [0.8027884  0.98398673 0.7750258  0.87794833 0.89127026]\n",
            "\t average f1 :  0.8662039041702645\n",
            "\t confusion matrix :  [[ 1785   281    45    61    78]\n",
            " [  186 40648   107   177    46]\n",
            " [   47   162   751    33    14]\n",
            " [   84   243    10  2345     8]\n",
            " [   95   121    18    36  1705]]\n",
            "Epoch #13\n",
            "Training..\n",
            "Training loss:  0.0020521767868625896\n",
            "Training metrics:\n",
            "\t accuracy :  0.9997554328837956\n",
            "\t f1 :  [0.99898518 0.99988313 0.99845098 0.99963497 0.99895967]\n",
            "\t average f1 :  0.9991827881196613\n",
            "\t confusion matrix :  [[  9844      9      0      4      2]\n",
            " [     3 166833      1      0      4]\n",
            " [     0     12   4512      0      1]\n",
            " [     1      2      0  10954      1]\n",
            " [     1      8      0      0   8162]]\n",
            "Validating..\n",
            "Validation loss:  0.1688051074743271\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9622091838813511\n",
            "\t f1 :  [0.80506558 0.98381971 0.77050026 0.87761307 0.88929889]\n",
            "\t average f1 :  0.8652595031388384\n",
            "\t confusion matrix :  [[ 1780   297    46    55    72]\n",
            " [  165 40708   107   143    41]\n",
            " [   47   173   747    29    11]\n",
            " [   80   282    11  2309     8]\n",
            " [  100   131    21    36  1687]]\n",
            "Epoch #14\n",
            "Training..\n",
            "Training loss:  0.0015731426602643398\n",
            "Training metrics:\n",
            "\t accuracy :  0.9998652835780681\n",
            "\t f1 :  [0.99944199 0.99993707 0.99922626 0.99981808 0.99932792]\n",
            "\t average f1 :  0.9995502656554827\n",
            "\t confusion matrix :  [[  9851      4      0      1      0]\n",
            " [     2 166853      2      0      3]\n",
            " [     0      5   4520      0      0]\n",
            " [     1      1      0  10992      1]\n",
            " [     3      4      0      0   8178]]\n",
            "Validating..\n",
            "Validation loss:  0.17678568725075042\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9626981216640182\n",
            "\t f1 :  [0.80939227 0.98385735 0.77893639 0.8749522  0.88982604]\n",
            "\t average f1 :  0.8673928486946725\n",
            "\t confusion matrix :  [[ 1758   316    42    61    73]\n",
            " [  130 40774    93   132    35]\n",
            " [   41   181   747    26    12]\n",
            " [   70   311    10  2288    11]\n",
            " [   95   140    19    33  1688]]\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "train_val_loop_lstm({\n",
        "    \"bidirectional\": True,\n",
        "    \"batch_size\": 512,\n",
        "    \"d_emb\": 64,\n",
        "    \"d_hidden\": 128,\n",
        "    \"num_epochs\": 15,\n",
        "    \"learning_rate\": 0.005,\n",
        "    \"l2\": 1e-6,\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vA-Yjqg7n0V"
      },
      "source": [
        "We were using bidirectional LSTMs. Please re-run the experiment with a regular (unidirectional) LSTM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "7wNrdvJ98ARB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa3a7830-4a25-4e3f-8af4-514b18259a08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameters: {'bidirectional': False, 'batch_size': 512, 'd_emb': 64, 'd_hidden': 128, 'num_epochs': 15, 'learning_rate': 0.005, 'l2': 1e-06}\n",
            "Loading data from eng.train\n",
            "14041 sentences loaded.\n",
            "Loading data from eng.val\n",
            "3490 sentences loaded.\n",
            "LSTM(\n",
            "  (words_vocab): Vocab()\n",
            "  (tags_vocab): Vocab()\n",
            "  (embedding): Embedding(20102, 64)\n",
            "  (lstm): LSTM(64, 128, batch_first=True)\n",
            "  (output_layer): Linear(in_features=128, out_features=5, bias=True)\n",
            ")\n",
            "Epoch #0\n",
            "Training..\n",
            "Training loss:  0.5816634650583621\n",
            "Training metrics:\n",
            "\t accuracy :  0.8245525674078581\n",
            "\t f1 :  [0.10227273 0.90999567 0.10731349 0.29895764 0.32216416]\n",
            "\t average f1 :  0.34814073537295764\n",
            "\t confusion matrix :  [[   711   7541    114    692    799]\n",
            " [  2748 159555    389   2997   1330]\n",
            " [   160   3423    292    273    382]\n",
            " [   226   7755     45   2696    231]\n",
            " [   202   5379     72    425   2096]]\n",
            "Validating..\n",
            "Validation loss:  0.3112829455307552\n",
            "Validation metrics:\n",
            "\t accuracy :  0.8999918510369556\n",
            "\t f1 :  [0.37643933 0.96378563 0.27279937 0.69053708 0.60415557]\n",
            "\t average f1 :  0.5815433956290917\n",
            "\t confusion matrix :  [[  850   861    40   228   271]\n",
            " [  563 40266    19   202   114]\n",
            " [  227   363   172    75   170]\n",
            " [  329   516     0  1755    90]\n",
            " [  297   388    23   133  1134]]\n",
            "Epoch #1\n",
            "Training..\n",
            "Training loss:  0.21602543581415107\n",
            "Training metrics:\n",
            "\t accuracy :  0.9310417061516824\n",
            "\t f1 :  [0.59535089 0.97846232 0.52396938 0.77666714 0.70304632]\n",
            "\t average f1 :  0.7154992103224185\n",
            "\t confusion matrix :  [[  5404   2111    425    837   1112]\n",
            " [   503 165457    182    568    245]\n",
            " [   740    950   1951    283    593]\n",
            " [   630   1781     98   8275    199]\n",
            " [   988    944    274    363   5585]]\n",
            "Validating..\n",
            "Validation loss:  0.25865150349480764\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9149248258159149\n",
            "\t f1 :  [0.50978032 0.97222974 0.60179104 0.76754295 0.7180593 ]\n",
            "\t average f1 :  0.7138806705531562\n",
            "\t confusion matrix :  [[ 1694   263    51   102   140]\n",
            " [ 1315 39526    61   119   143]\n",
            " [  289   121   504    30    63]\n",
            " [  624   145    10  1854    57]\n",
            " [  474    91    42    36  1332]]\n",
            "Epoch #2\n",
            "Training..\n",
            "Training loss:  0.12093994324957882\n",
            "Training metrics:\n",
            "\t accuracy :  0.962686083658528\n",
            "\t f1 :  [0.77199816 0.99051915 0.73734996 0.89663108 0.82020709]\n",
            "\t average f1 :  0.8433410877154776\n",
            "\t confusion matrix :  [[  7532    852    376    409    698]\n",
            " [   377 166221    141    283    133]\n",
            " [   481    437   3133    140    309]\n",
            " [   434    584    108   9741    115]\n",
            " [   822    375    240    173   6535]]\n",
            "Validating..\n",
            "Validation loss:  0.21897532897336142\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9304689728232083\n",
            "\t f1 :  [0.58368806 0.97731465 0.68274384 0.79275053 0.75283213]\n",
            "\t average f1 :  0.757865843356072\n",
            "\t confusion matrix :  [[ 1714   273    64    62   137]\n",
            " [  872 40001    98    48   145]\n",
            " [  177   138   637    11    44]\n",
            " [  494   196    20  1859   121]\n",
            " [  366    87    40    20  1462]]\n",
            "Epoch #3\n",
            "Training..\n",
            "Training loss:  0.07724456033772892\n",
            "Training metrics:\n",
            "\t accuracy :  0.976296111665005\n",
            "\t f1 :  [0.84596975 0.994978   0.82222981 0.94551765 0.87311216]\n",
            "\t average f1 :  0.8963614745442848\n",
            "\t confusion matrix :  [[  8307    455    299    275    549]\n",
            " [   257 166523     90    118     78]\n",
            " [   316    262   3610     92    249]\n",
            " [   243    227     71  10352     62]\n",
            " [   631    194    182    105   7053]]\n",
            "Validating..\n",
            "Validation loss:  0.22049686525549209\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9312838691276535\n",
            "\t f1 :  [0.58896104 0.97773739 0.70844397 0.80666245 0.7632312 ]\n",
            "\t average f1 :  0.7690072102275658\n",
            "\t confusion matrix :  [[ 1814   202    46    51   137]\n",
            " [ 1035 39812   103    62   152]\n",
            " [  185   103   667    10    42]\n",
            " [  512   108    21  1913   136]\n",
            " [  364    48    39    17  1507]]\n",
            "Epoch #4\n",
            "Training..\n",
            "Training loss:  0.052387186222606234\n",
            "Training metrics:\n",
            "\t accuracy :  0.9842817022253039\n",
            "\t f1 :  [0.8928517  0.99702287 0.87651822 0.97047836 0.91033716]\n",
            "\t average f1 :  0.9294416628754405\n",
            "\t confusion matrix :  [[  8787    295    232    165    400]\n",
            " [   179 166610     53     60     43]\n",
            " [   226    165   3897     66    168]\n",
            " [   129     90     39  10651     40]\n",
            " [   483    110    149     59   7371]]\n",
            "Validating..\n",
            "Validation loss:  0.21608841844967433\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9345841991606568\n",
            "\t f1 :  [0.66651736 0.97865104 0.7245409  0.82294056 0.65712161]\n",
            "\t average f1 :  0.7699542935332169\n",
            "\t confusion matrix :  [[ 1488   244    38    56   424]\n",
            " [  312 39996    72    52   732]\n",
            " [   78   118   651    11   149]\n",
            " [  202   170    14  1973   331]\n",
            " [  135    45    15    13  1767]]\n",
            "Epoch #5\n",
            "Training..\n",
            "Training loss:  0.03791208848081253\n",
            "Training metrics:\n",
            "\t accuracy :  0.9884852262810123\n",
            "\t f1 :  [0.9181888  0.99785234 0.90888664 0.98221614 0.93274639]\n",
            "\t average f1 :  0.947978063249167\n",
            "\t confusion matrix :  [[  8973    220    188    105    344]\n",
            " [   145 166800     42     29     43]\n",
            " [   176    115   4045     43    130]\n",
            " [    77     47     22  10770     35]\n",
            " [   344     77     95     32   7628]]\n",
            "Validating..\n",
            "Validation loss:  0.2406226864882878\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9314875932037648\n",
            "\t f1 :  [0.60748592 0.97703087 0.71991247 0.81451951 0.74324642]\n",
            "\t average f1 :  0.7724390386970618\n",
            "\t confusion matrix :  [[ 1834   169    36    49   162]\n",
            " [ 1010 39708    83    68   295]\n",
            " [  181    87   658     7    74]\n",
            " [  440   124    16  1941   169]\n",
            " [  323    31    28    11  1582]]\n",
            "Epoch #6\n",
            "Training..\n",
            "Training loss:  0.029300331241554685\n",
            "Training metrics:\n",
            "\t accuracy :  0.9912475597517563\n",
            "\t f1 :  [0.93630671 0.99833128 0.92837126 0.98893563 0.95003364]\n",
            "\t average f1 :  0.9603957015742053\n",
            "\t confusion matrix :  [[  9195    175    162     61    264]\n",
            " [   127 166616     30     20     30]\n",
            " [   146     93   4141     20    104]\n",
            " [    53     35     14  10815     16]\n",
            " [   263     47     70     23   7767]]\n",
            "Validating..\n",
            "Validation loss:  0.22568861501557486\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9346453163834902\n",
            "\t f1 :  [0.67138264 0.97754796 0.73170732 0.82795914 0.6756238 ]\n",
            "\t average f1 :  0.7768441718559921\n",
            "\t confusion matrix :  [[ 1566   204    43    65   372]\n",
            " [  438 39795   100   137   694]\n",
            " [   90    95   690    17   115]\n",
            " [  184   128    17  2067   294]\n",
            " [  137    32    29    17  1760]]\n",
            "Epoch #7\n",
            "Training..\n",
            "Training loss:  0.02251696262370657\n",
            "Training metrics:\n",
            "\t accuracy :  0.993109229441204\n",
            "\t f1 :  [0.94976428 0.99880792 0.94208665 0.99354721 0.95664421]\n",
            "\t average f1 :  0.9681700539741932\n",
            "\t confusion matrix :  [[  9368    131    141     30    224]\n",
            " [    97 166736     27     12     25]\n",
            " [   111     59   4262      9     97]\n",
            " [    29     16     11  10855     15]\n",
            " [   228     31     69     19   7811]]\n",
            "Validating..\n",
            "Validation loss:  0.20623163453170232\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9425701829442203\n",
            "\t f1 :  [0.67754596 0.9802622  0.75055432 0.84084934 0.72924989]\n",
            "\t average f1 :  0.7956923426248015\n",
            "\t confusion matrix :  [[ 1640   276    32    60   242]\n",
            " [  417 40228    48    86   385]\n",
            " [  109   129   677    14    78]\n",
            " [  194   221    13  2079   183]\n",
            " [  231    58    27    16  1643]]\n",
            "Epoch #8\n",
            "Training..\n",
            "Training loss:  0.018924391518036526\n",
            "Training metrics:\n",
            "\t accuracy :  0.994087799913188\n",
            "\t f1 :  [0.95539203 0.99904172 0.95546378 0.99461679 0.96000489]\n",
            "\t average f1 :  0.972903841587474\n",
            "\t confusion matrix :  [[  9413    109     92     24    240]\n",
            " [    75 166805     20      8     22]\n",
            " [    97     49   4280      9     78]\n",
            " [    28     13      7  10901      9]\n",
            " [   214     24     47     20   7849]]\n",
            "Validating..\n",
            "Validation loss:  0.22704528910773142\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9388216599437722\n",
            "\t f1 :  [0.67462932 0.97910879 0.72688629 0.83598102 0.71094711]\n",
            "\t average f1 :  0.7855105065780639\n",
            "\t confusion matrix :  [[ 1638   217    48    35   312]\n",
            " [  467 40001    96    76   524]\n",
            " [  103   113   684     8    99]\n",
            " [  246   168    16  2026   234]\n",
            " [  152    46    31    12  1734]]\n",
            "Epoch #9\n",
            "Training..\n",
            "Training loss:  0.015118076017609349\n",
            "Training metrics:\n",
            "\t accuracy :  0.9952631026677526\n",
            "\t f1 :  [0.9627784  0.99919012 0.96523381 0.99643412 0.96921099]\n",
            "\t average f1 :  0.978569488923123\n",
            "\t confusion matrix :  [[  9467    102     79     16    179]\n",
            " [    68 166556     20      5     18]\n",
            " [    84     33   4345      5     50]\n",
            " [    24      6      3  10898      7]\n",
            " [   180     18     39     12   7917]]\n",
            "Validating..\n",
            "Validation loss:  0.23380268258707865\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9374770810414375\n",
            "\t f1 :  [0.68725702 0.97796471 0.71694215 0.83782712 0.69527041]\n",
            "\t average f1 :  0.783052280768038\n",
            "\t confusion matrix :  [[ 1591   214    43    59   343]\n",
            " [  385 39877   135   147   620]\n",
            " [   90   103   694    15   105]\n",
            " [  176   152    23  2113   226]\n",
            " [  138    41    34    20  1742]]\n",
            "Epoch #10\n",
            "Training..\n",
            "Training loss:  0.012704315905769667\n",
            "Training metrics:\n",
            "\t accuracy :  0.9960579889264872\n",
            "\t f1 :  [0.97017189 0.99927589 0.97454545 0.99671892 0.97260526]\n",
            "\t average f1 :  0.9826634823183401\n",
            "\t confusion matrix :  [[  9595     82     56     12    161]\n",
            " [    55 166980     22     12     25]\n",
            " [    51     26   4422      3     38]\n",
            " [    16      7      2  10936      7]\n",
            " [   157     13     33     13   7935]]\n",
            "Validating..\n",
            "Validation loss:  0.40545314124652315\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9311616346819868\n",
            "\t f1 :  [0.65535478 0.97148014 0.72266974 0.58853898 0.81157895]\n",
            "\t average f1 :  0.7499245156341642\n",
            "\t confusion matrix :  [[ 1487   577    28    17   141]\n",
            " [  156 40910    49    13    36]\n",
            " [   86   252   628     5    36]\n",
            " [  398  1075     7  1140    70]\n",
            " [  161   244    19     9  1542]]\n",
            "Epoch #11\n",
            "Training..\n",
            "Training loss:  0.029547616218527157\n",
            "Training metrics:\n",
            "\t accuracy :  0.9905856015670675\n",
            "\t f1 :  [0.95433231 0.9967554  0.955015   0.96406399 0.9637951 ]\n",
            "\t average f1 :  0.9667923599628839\n",
            "\t confusion matrix :  [[  9362    146     88     50    207]\n",
            " [   108 166044     53    355     41]\n",
            " [    61     78   4299     15     61]\n",
            " [    63    259     15  10637     20]\n",
            " [   173     41     34     16   7893]]\n",
            "Validating..\n",
            "Validation loss:  0.21131731144019536\n",
            "Validation metrics:\n",
            "\t accuracy :  0.948559670781893\n",
            "\t f1 :  [0.7209697  0.97921868 0.72501295 0.85286379 0.78470919]\n",
            "\t average f1 :  0.8125548619802714\n",
            "\t confusion matrix :  [[ 1487   414    55    53   241]\n",
            " [  159 40594   110    64   237]\n",
            " [   52   178   700     9    68]\n",
            " [   69   425    19  2107    70]\n",
            " [  108   136    40    18  1673]]\n",
            "Epoch #12\n",
            "Training..\n",
            "Training loss:  0.010834956334696876\n",
            "Training metrics:\n",
            "\t accuracy :  0.996634153385016\n",
            "\t f1 :  [0.97391216 0.99937235 0.97890764 0.99690149 0.97750686]\n",
            "\t average f1 :  0.9853200988924049\n",
            "\t confusion matrix :  [[  9613     72     47     11    143]\n",
            " [    56 167186     12      6     18]\n",
            " [    44     24   4409      4     34]\n",
            " [    17     11      3  10939      9]\n",
            " [   125     11     22      7   8018]]\n",
            "Validating..\n",
            "Validation loss:  0.22414465887205942\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9475817952165587\n",
            "\t f1 :  [0.71956675 0.97894458 0.73196419 0.8394401  0.78322337]\n",
            "\t average f1 :  0.8106277986838928\n",
            "\t confusion matrix :  [[ 1528   411    46    46   219]\n",
            " [  189 40589    93    57   236]\n",
            " [   66   173   695     8    65]\n",
            " [   93   450    21  2039    87]\n",
            " [  121   137    37    18  1662]]\n",
            "Epoch #13\n",
            "Training..\n",
            "Training loss:  0.00871257194214397\n",
            "Training metrics:\n",
            "\t accuracy :  0.9973080220941594\n",
            "\t f1 :  [0.978957   0.99955418 0.98492908 0.99762622 0.97990594]\n",
            "\t average f1 :  0.988194484026498\n",
            "\t confusion matrix :  [[  9630     46     34     12    127]\n",
            " [    39 167033     14      2     17]\n",
            " [    24     12   4444      4     27]\n",
            " [    15      6      3  10927      4]\n",
            " [   117     13     18      6   8022]]\n",
            "Validating..\n",
            "Validation loss:  0.2319610459463937\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9489060017112823\n",
            "\t f1 :  [0.726451   0.97934188 0.7311828  0.83806001 0.79137388]\n",
            "\t average f1 :  0.8132819116026718\n",
            "\t confusion matrix :  [[ 1527   436    46    48   193]\n",
            " [  155 40699    68    61   181]\n",
            " [   66   197   680    10    54]\n",
            " [   75   465    20  2039    91]\n",
            " [  131   154    39    18  1633]]\n",
            "Epoch #14\n",
            "Training..\n",
            "Training loss:  0.008001966539908338\n",
            "Training metrics:\n",
            "\t accuracy :  0.9973220830902263\n",
            "\t f1 :  [0.9780431  0.9995718  0.98651636 0.99785848 0.97992902]\n",
            "\t average f1 :  0.9883837509237914\n",
            "\t confusion matrix :  [[  9666     53     30     12    122]\n",
            " [    36 166906     14      1     15]\n",
            " [    24     14   4463      3     22]\n",
            " [    16      2      4  10950      5]\n",
            " [   141      8     11      4   8007]]\n",
            "Validating..\n",
            "Validation loss:  0.2480870293719428\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9493745670863383\n",
            "\t f1 :  [0.71729141 0.97899714 0.72239748 0.84398767 0.8064843 ]\n",
            "\t average f1 :  0.8138315995119179\n",
            "\t confusion matrix :  [[ 1483   475    56    50   186]\n",
            " [  135 40786    86    46   111]\n",
            " [   55   211   687     8    46]\n",
            " [   63   520    16  2053    38]\n",
            " [  149   166    50    18  1592]]\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "## TODO: Re-run with unidirectional LSTMs\n",
        "## Keep other hyperparameters fixed\n",
        "train_val_loop_lstm({\n",
        "    \"bidirectional\": False,\n",
        "    \"batch_size\": 512,\n",
        "    \"d_emb\": 64,\n",
        "    \"d_hidden\": 128,\n",
        "    \"num_epochs\": 15,\n",
        "    \"learning_rate\": 0.005,\n",
        "    \"l2\": 1e-6,\n",
        "})\n",
        "## END"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8UChDyKaPBs"
      },
      "source": [
        "### Questions **(2 points)**\n",
        "\n",
        "(a) How does the final performance of LSTMs compare to FFNNs? Is it better? What is a possible explanation?\n",
        "\n",
        "LSTMs did a bit better than FFNNs. LSTMs are good at understanding sequences, which helped them perform better.\n",
        "\n",
        "\n",
        "(b) How does bidirectional LSTMs compare to unidirectional LSTMs? Why?\n",
        "\n",
        "\n",
        "Bidirectional LSTMs performed better than unidirectional LSTMs. Bidirectional LSTMs can capture context from both past and future, enhancing their understanding of sequences and improving performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMEWxkN_bpIT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}